# DataClear.py
# å¯æ‰‹åŠ¨æ‰§è¡Œã€‚
import os
import re
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin  # æ‹¼æ¥urlç”¨

import setting
import adsolver
from ujson import load_json

import base64
import hashlib
from Crypto.Cipher import AES
# import requests
from curl_cffi import requests
from loguru import logger


# è°ƒç”¨æ¸…ç†æ¨¡å—ä¼šæŠŠæ‰€æœ‰å›è½¦ç­‰ç­‰ä¸œè¥¿å…¨éƒ¨åˆ é™¤ï¼Œå› æ­¤å•ç‹¬å†™è¿™ä¸ªå‡½æ•°åˆ é™¤å¹¿å‘Š
def only_delete_ad(content, BAN_TEXT_LIST):
    ad_count = 0
    for ban in BAN_TEXT_LIST:
        if ban in content:
            content = content.replace(ban, '')
            ad_count += 1
    logger.info(f'>>å¤„ç†å¹¿å‘Š{ad_count}æ¡')
    return content


def __get_md5(self, url) -> str:
    img_url = self.__base_url + '/toimg/data/' + url
    response = requests.get(img_url)
    with open('temp.png', 'wb') as file_obj:
        file_obj.write(response.content)
    file = open("temp.jpg", "rb")
    return hashlib.md5(file.read()).hexdigest()


def get_HTML(path):
    if not os.path.exists(path):
        logger.info('HTMLæ–‡ä»¶ä¸å­˜åœ¨')
    else:
        with open(path, 'r', encoding='utf-8') as f:
            HTML = f.read()
    return HTML


def Save_HTML(ROOTPATH, BOOKNAME, filename, HTML, PathType):
    suffix = '.txt'  # åç¼€
    if PathType == 'One_file':
        path = ROOTPATH
        suffix = ''  # ä¼ å…¥å•ä¸ªæ–‡ä»¶çš„æ—¶å€™ï¼Œå·²ç»å¸¦åç¼€äº†ã€‚
    else:
        path = os.path.join(ROOTPATH, BOOKNAME)

    if not os.path.exists(path):
        os.makedirs(path)

    if setting.SAVE_AS_ONE_FILE:
        with open(os.path.join(path, BOOKNAME) + suffix, 'a', encoding='utf-8') as f:
            f.write(filename + '\n  ' + HTML + '\n \n')
    else:
        with open(os.path.join(path, filename) + suffix, 'w', encoding='utf-8') as f:
            f.write(HTML)

    logger.info(f'å·²ä¿å­˜è‡³æ–‡ä»¶{path}')


def get_ad_list():
    BAN_TEXT_LIST = []
    with open(setting.AD_LIST_PATH, 'r', encoding='UTF-8') as f:
        for ban in f.read().split('\n'):
            BAN_TEXT_LIST.append(ban)
    logger.info('å±è”½åˆ—è¡¨è·å–æˆåŠŸ')
    return BAN_TEXT_LIST


# æ ¹æ®é€†å‘sojson.v5åŠ å¯†å¾—åˆ°çš„æ’åºæ–¹æ³•ï¼Œç”¨äºè·å–æ­£ç¡®çš„æ–‡æœ¬é¡ºåºã€‚
def get_true_HTML(HTML, ns):
    soup = BeautifulSoup(HTML, 'lxml')
    HTML = soup.find(id='chapter')
    # åŠ  'utf-8'ï¼Œé¿å…ç”Ÿæˆçš„å­—ç¬¦ä¸²å¸¦æœ‰bå‰ç¼€
    IncList = str(base64.b64decode(ns), 'utf-8').split(',')
    BookList = str(HTML).split("<br/><br/>")
    FNo = IncList[0]
    Booklength = len(BookList)
    Result = ''
    for I in range(1, Booklength + 1):
        Result += BookList[int(IncList[I]) - int(FNo)] + "<br/><br/>"
    return Result


# åˆ¤æ–­é¡µé¢æ˜¯å¦éœ€è¦é‡æ–°æ’åº
def check_NeedSort(HTML):
    soup = BeautifulSoup(HTML, 'lxml')
    if (len(soup.find_all(id="ad")) > 0) and (
            "_ii_rr(ns);" in list(
        SC.get_text() for SC in soup.find_all(name="script"))):
        return True
    else:
        return False


# åˆ¤æ–­é¡µé¢æ˜¯å¦éœ€è¦è¿›è¡ŒAESè§£å¯†
def check_NeedDecipher(HTML):
    soup = BeautifulSoup(HTML, 'lxml')
    if re.search('secret', HTML):
        return True
    else:
        return False


# å¯¹HTMLè¿›è¡ŒAESè§£å¯†ã€‚
def get_true_HTML_AES(secret_Data, secret_pw, ad_list=[]):
    AText = ''
    data = secret_Data[1:len(secret_Data) - 1]
    pw = secret_pw[1:len(secret_pw) - 1]

    pw = hashlib.md5(pw.encode(encoding='UTF-8')).hexdigest()
    IV = bytes(pw[0:16].encode('utf-8'))
    KEY = bytes(pw[-16:].encode('utf-8'))

    decode = base64.b64decode(data)
    cryptor = AES.new(KEY, AES.MODE_CBC, IV)
    AText = cryptor.decrypt(decode)
    AText = AText.decode("utf-8")
    # ç¬¬äºŒé¡µè§£å¯†å‡ºçš„æ–‡æœ¬æ²¡æœ‰å›¾ç‰‡å’ŒdivèŠ‚ç‚¹åœ¨ï¼Œé¡ºä¾¿æŠŠè¿™äº›ä¸œè¥¿æ¸…æ´—äº†
    AText = re.sub('<br/>', '', AText)
    AText = re.sub('&nbsp;', '', AText)
    AText = re.sub('\xa0', '', AText)
    AText = re.sub('', '', AText)
    AText = re.sub('', '', AText)
    AText = re.sub('', '', AText)
    AText = re.sub('', '', AText)
    AText = re.sub('', '', AText)
    AText = re.sub('', '', AText)
    AText = re.sub('', '', AText)
    if ad_list:
        AText = only_delete_ad(AText, ad_list)  # å¹¿å‘Šæ¸…ç†
    return AText


def clear(HTML, filename, img_json, ad_list):
    soup = BeautifulSoup(HTML, 'lxml')
    logger.info(f'å¾…æ¸…æ´—æ–‡ä»¶{filename}è·å–æˆåŠŸ')

    if re.search('.*-@', filename, re.S):
        filename = re.search('.*-@', filename, re.S).group()
        filename = filename.replace('-@', '')

    # æ³¨æ„ï¼Œè¿™é‡Œçš„æ¸…æ´—åˆ¤æ–­æ¡ä»¶æœªåŠ å…¥å¹¿å‘Šæ¸…æ´—çš„åˆ¤æ–­
    if (len(soup.find_all(name='div')) < 1) and (
            len(soup.find_all(name='img')) < 1):
        logger.info('è¯¥æ–‡ä»¶æ— éœ€æ¸…æ´—')
        HTML = only_delete_ad(HTML, ad_list)  # å¹¿å‘Šæ¸…ç†
        return {'filename': filename, 'text': HTML.strip()}

    for I in list((5 - i) for i in range(1, 5)):  # é¿å…é—ç•™å¤§é‡ç©ºæ ¼è€Œåšçš„å¤„ç†
        SpaceStr = '&nbsp;' * I
        HTML = re.sub(SpaceStr, '', HTML)  # æ›¿æ¢&nbsp;
        SpaceStr = '\xa0' * I
        HTML = re.sub(SpaceStr, '', HTML)  # æ›¿æ¢\xa0

    HTML = HTML.replace('<br>', '')  # æ›¿æ¢æ¢è¡Œ
    HTML = HTML.replace('<br/>', '')  # æ›¿æ¢æ¢è¡Œ
    HTML = HTML.replace('<br >', '')  # æ›¿æ¢æ¢è¡Œ
    HTML = HTML.replace('<br />', '')  # æ›¿æ¢æ¢è¡Œ
    HTML = HTML.replace('\n', '')  # æ›¿æ¢æ¢è¡Œ

    HTML_imgs = []
    soup_as = soup.find_all(name='img')
    if len(soup_as) > 0:  # æœ‰çš„æ–‡æœ¬ä¸å«imgï¼Œæ’é™¤ã€‚
        for soup_a in soup_as:
            # img_Url=urljoin(setting.DOMAIN_NAME, soup_a.attrs['src'])
            # 2024.03.27 - èŠ‚ç‚¹æ”¹å˜ï¼Œä¸æ­¢æ˜¯å«æœ‰ <img srcï¼Œè¿˜åŒ…æ‹¬äº† <img data-cfsrc
            # å…¨æ–°èŠ‚ç‚¹1ï¼š
            # <img data-cfsrc="/toimg/data/5798715549.png" style="display:none;visibility:hidden;" /><noscript><img src="/toimg/data/5798715549.png" /></noscript>
            # å…¨æ–°èŠ‚ç‚¹2:
            # <img data-cfsrc="/toimg/data/4208731327.png" style="display:none;visibility:hidden;"/><noscript>æœª</noscript>
            # å…¨æ–°èŠ‚ç‚¹3ï¼š
            # <img data-cfsrc="/toimg/data/3043454467.png" style="display:none;visibility:hidden;<noscript><img src="/toimg/data/3043454467.png</noscript>
            if ('src' in str(soup_a)) and ('data-cfsrc' not in str(soup_a)):
                img_Url_1 = urljoin(setting.DOMAIN_NAME, soup_a.attrs['src'])
                img_aName_1 = re.search('/(\w)*?.png', soup_a.attrs['src'], re.S).group()  # img_Url
                img_aName_1 = img_aName_1.replace('/', '')
                img_dist_1 = {'img_Url': img_Url_1,
                            'img_aName': img_aName_1,
                            'text': ''}
                # å»é‡
                if not (img_dist_1 in HTML_imgs):
                    HTML_imgs.append(img_dist_1)

            if 'data-cfsrc' in str(soup_a):
                img_Url_2 = urljoin(setting.DOMAIN_NAME, soup_a.attrs['data-cfsrc'])
                img_aName_2 = re.search('/(\w)*?.png', soup_a.attrs['data-cfsrc'], re.S).group()  # img_Url
                img_aName_2 = img_aName_2.replace('/', '')
                img_dist_2 = {'img_Url': img_Url_2,
                            'img_aName': img_aName_2,
                            'text': ''}
                # å»é‡
                if not (img_dist_2 in HTML_imgs):
                    HTML_imgs.append(img_dist_2)

        # æ³¨å…¥æ–‡æœ¬ï¼š
        for HTML_img in HTML_imgs:
            for img2text in img_json['texts']:
                if HTML_img['img_aName'] == img2text['imgName']:
                    HTML_img['text'] = img2text['text']

        uncheck_img_list = []

        # æ›¿æ¢æ–‡æœ¬ï¼š
        for HTML_img in HTML_imgs:
            if HTML_img['text'] != '':
                img_aName = HTML_img['img_aName']
                img_text = HTML_img['text']
                # 2024.03.27 æ–°åŠ å…¥
                HTML = HTML.replace(img_aName, img_text)

                HTML = HTML.replace('<noscript>', '')
                HTML = HTML.replace('</noscript>', '')

                HTML = HTML.replace('<img src="/toimg/data/', '')

                noscript_is_str_1 = f'<img data-cfsrc="/toimg/data/{img_text}" style="display:none;visibility:hidden;'
                HTML = HTML.replace(noscript_is_str_1, '')

                HTML = HTML.replace(img_aName, img_text)

                # OLD
                # HTML = HTML.replace(f'<img src="/toimg/data/{img_aName}">', HTML_img['text'])
                # HTML = HTML.replace(f'<img src="/toimg/data/{img_aName}"/>', HTML_img['text'])
                # HTML = HTML.replace(f'<img src="/toimg/data/{img_aName}" />', HTML_img['text'])
            else:
                logger.info(f"å›¾ç‰‡{HTML_img['img_aName']}æ‰¾ä¸åˆ°å¯¹åº”çš„æ–‡æœ¬ï¼Œå°†å…¶ç½®å…¥æœªå¤„ç†å›¾ç‰‡txtæ–‡ä»¶ä¸­")
                uncheck_img_list.append(HTML_img['img_aName'])

        try:
            for _img_name in uncheck_img_list:
                with open("./tool/uncheck_img.txt", 'a', encoding='utf-8') as f:
                    f.write(f"{_img_name}\n")
        except Exception as e:
            logger.error(f"æœªå¤„ç†å›¾ç‰‡jsonä¿å­˜å¤±è´¥")

        # print(HTML_imgs)
        # print(HTML)

    soup_div = BeautifulSoup(HTML, 'lxml')  # æ¸…é™¤éšè—èŠ‚ç‚¹
    del_divs = soup_div.find_all(name='div', attrs={'id': 'chapter'})
    if len(del_divs) > 0:
        for del_div in del_divs:
            HTML = HTML.replace(str(del_div), '')

    div_tags = re.findall('<div.*?>', HTML, re.S)  # æå–å‡ºæ‰€æœ‰çš„<div æ ‡ç­¾>
    if len(div_tags) > 0:
        for div_tag in div_tags:
            HTML = HTML.replace(str(div_tag), '')  # æ¸…é™¤<div æ ‡ç­¾>
            HTML = HTML.replace('</div>', '')

    HTML = adsolver.solve_ad_text(HTML, ad_list)  # å¹¿å‘Šæ¸…ç†

    ### 2024-1-3 æ–°å¢ï¼š
    pattern = r'</p><!--baidu-->(.*?)</center>'
    # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹
    matches = re.findall(pattern, HTML)
    # è¾“å‡ºç»“æœ
    img_ele_list = []
    for match in matches:
        img_ele_list.append(str(match))
    img_ele_list = list(set(img_ele_list))  # éœ€è¦æ›¿æ¢çš„å›¾ç‰‡èŠ‚ç‚¹å­—ç¬¦ä¸²åˆ—è¡¨
    for img_ele in img_ele_list:
        HTML = HTML.replace(str(img_ele), "")
        logger.info(f"å°†èŠ‚ç‚¹ï¼š{str(img_ele)} æ›¿æ¢ä¸ºç©º")
    logger.info(HTML[:200])
    HTML = HTML.replace("<p>", "")
    HTML = HTML.replace("</p><!--baidu--></center>", "")
    ### 2024-1-3 æ–°å¢ï¼š

    # æ¸…æ´—æ®‹ç•™ç¬¦å·
    HTML = HTML.replace('" />', '')
    HTML = HTML.replace('"/>', '')
    HTML = HTML.replace('">', '')

    logger.info(f'æ–‡ä»¶ã€{filename}ã€‘æ¸…æ´—æˆåŠŸ')

    # print(HTML)
    return {'filename': filename, 'text': HTML.strip()}  # è¿”å›ç« èŠ‚åï¼Œè¿”å›çš„æ–‡æœ¬åˆ é™¤å‰åç©ºç™½


# æ¸…æ´—å¯¹åº”ç›®å½•ä¸‹çš„æ–‡æœ¬
def Start_clear():
    Strat = time.time()
    ad_list = get_ad_list()  # è·å–å¹¿å‘Šå±è”½åˆ—è¡¨
    img_json = load_json(setting.TEXT_JSON_PATH)  # è·å–å›¾ç‰‡å¯¹åº”æ–‡æœ¬å­—å…¸
    root = setting.HTML_PATH  # -æ ¹ç›®å½•
    Booklist = os.listdir(root)  # è·å–æ–‡ä»¶å¤¹å†…ä¹¦åç›®å½•åˆ—è¡¨

    for BookName in Booklist:  # æ³¨æ„è¿™é‡Œè·å–åˆ°çš„æŒ‡ç¤ºæ–‡ä»¶å¤¹å-BookName ï¼šä¹¦å
        if os.path.isdir(os.path.join(root, BookName)):  # åˆ¤æ–­è·¯å¾„æ˜¯å¦ç›®å½•
            Chapterlist = os.listdir(os.path.join(root, BookName))  # è·å–ä¹¦åæ–‡ä»¶å¤¹å†…æ–‡æœ¬åˆ—è¡¨
            for Chapter in Chapterlist:  # Chapter ï¼šç« èŠ‚å
                if os.path.join(root, BookName, Chapter).endswith('.txt'):  # åˆ¤æ–­æ˜¯å¦txtæ–‡æœ¬
                    HTML = get_HTML(os.path.join(root, BookName, Chapter))
                    Data = clear(HTML, Chapter, img_json, ad_list)
                    Save_HTML(setting.CLEAR_PATH, BookName, Data['filename'],
                              Data['text'], PathType='Many_file')
        else:
            if BookName.endswith('.txt'):  # åˆ¤æ–­æ˜¯å¦txtæ–‡æœ¬
                HTML = get_HTML(os.path.join(root, BookName))
                Data = clear(HTML, BookName, img_json, ad_list)
                Save_HTML(setting.CLEAR_PATH, BookName, Data['filename'],
                          Data['text'], PathType='One_file')  # '''
    logger.info(f'æ¸…æ´—è€—æ—¶ï¼š{str(time.time() - Strat)}ç§’')


def main():
    Start_clear()


if __name__ == '__main__':
    main()
